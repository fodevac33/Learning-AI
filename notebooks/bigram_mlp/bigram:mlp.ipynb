{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d988689e",
   "metadata": {},
   "source": [
    "# Modeling language with Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "687ae208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1013\n",
      "\n",
      "First 10 characters in vocabulary:\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(']\n",
      "\n",
      "First 5 bigrams as character pairs:\n",
      "' ' -> '\n",
      "'\n",
      "'\n",
      "' -> ' '\n",
      "' ' -> '='\n",
      "'=' -> ' '\n",
      "' ' -> 'V'\n",
      "\n",
      "Total number of bigrams: 10918891\n",
      "\n",
      "Most common characters:\n",
      "'<space>': 2088677\n",
      "'e': 990626\n",
      "'t': 690297\n",
      "'a': 685507\n",
      "'n': 593094\n",
      "'i': 588695\n",
      "'o': 585162\n",
      "'r': 533179\n",
      "'s': 514154\n",
      "'h': 387532\n"
     ]
    }
   ],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "train_raw = \"./wikitext-2-raw/wiki.train.txt\"\n",
    "val_raw = \"./wikitext-2-raw/wiki.val.txt\"\n",
    "test_raw = \"./wikitext-2-raw/wiki.test.txt\"\n",
    "\n",
    "train_text = read_file(train_raw)\n",
    "\n",
    "chars = sorted(list(set(train_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create our character mappings\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create bigrams as simple tuples of indices\n",
    "bigrams = []\n",
    "for i in range(len(train_text) - 1):\n",
    "    input_char = train_text[i]\n",
    "    target_char = train_text[i + 1]\n",
    "    bigrams.append([char_to_idx[input_char], char_to_idx[target_char]])\n",
    "\n",
    "# Let's look at what we've created\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"\\nFirst 10 characters in vocabulary:\")\n",
    "print(chars[:10])\n",
    "\n",
    "print(f\"\\nFirst 5 bigrams as character pairs:\")\n",
    "for i in range(5):\n",
    "    in_idx, out_idx = bigrams[i]\n",
    "    print(f\"'{idx_to_char[in_idx]}' -> '{idx_to_char[out_idx]}'\")\n",
    "\n",
    "print(f\"\\nTotal number of bigrams: {len(bigrams)}\")\n",
    "\n",
    "# Optional: let's also see the distribution of characters\n",
    "from collections import Counter\n",
    "char_counts = Counter(train_text)\n",
    "print(\"\\nMost common characters:\")\n",
    "for char, count in char_counts.most_common(10):\n",
    "    if char.isspace():\n",
    "        char_display = '<space>'\n",
    "    elif char == '\\n':\n",
    "        char_display = '<newline>'\n",
    "    else:\n",
    "        char_display = char\n",
    "    print(f\"'{char_display}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96f259",
   "metadata": {},
   "source": [
    "## Rationale\n",
    "\n",
    "We have a text corpus $T$ of length $n$ characters, from this corpus we derive a dataset $\\mathbf{X}$ composed of all contiguous bigrams $[x_{i}, x_{i+1}]$ where $i \\in 1,\\dots,n-1$ (therefore $|X| = n-1$) and $x_{i} \\in V$, where $V$ is the set of all distinct characters in $T$ and $|V|=m$. For any character $x_{i}$ the corpus $T$ encodes an underlying, true distribution for $x_{i+1}$ :\n",
    "$$\n",
    "P(x_{i+1}|x_{i})\n",
    "$$\n",
    "Our objective is to approximate that distribution as best we can by using a model composed of parameters $\\Theta$ learned from our dataset $X$. The model will use character $x_{i}$ to predict $x_{i+1}$ ideally we would get:\n",
    "$$\n",
    "P(x_{i+1}|x_{i}) \\approx P(x_{i+1}|x_{i}, \\Theta)\\in \\mathbb{R}^{m}\n",
    "$$\n",
    "More practically, we can say that our model is actually a function $f_{\\Theta}$ (parameterized by $\\Theta$) where:\n",
    "$$\n",
    "\\text{softmax}(f_{\\Theta}(x_{i})) = P(x_{i+1}|x_{i},\\Theta)\n",
    "$$\n",
    "Where softmax is used to turn our final $m$ logits into a valid probability distribution.\n",
    "\n",
    "Now, this is an optimization problem. To be precise, we want to minimize the difference between the real distribution and the distribution we're estimating with out model $f_{\\Theta}$. For this, we can pull from the idea of cross-entropy which is the measure that we need:\n",
    "$$\n",
    "H(p,q)=-\\sum p(x)\\log q(x)\n",
    "$$\n",
    "Applying it to our problem we would get a specific number for the cross-entropy between the relevant distribution this will be our loss and it will be what we want to minimize:\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{i=1}^{n-1} P(x_{i+1}|x_{i})\\log P(x_{i+1}|x_{i},\\Theta)\n",
    "$$\n",
    "Obviously, we don't have $P(x_{i+1}|x_{i})$, if we did then problem solved we could just use that, instead all we have is our dataset $X$ which tells us a lot about the real distribution for any one *specific* example.\n",
    "\n",
    "We have all possible bigrams, therefore for any character $x_{i}$ we know what $x_{x+1}$ actually is, therefore the real distribution $P(x|x_{i})$ has a spike of probability $1$ when $x=x_{i+1}$ and a total of $0$ probability mass when $x=c$, c being any other character other than $x_{i+1}$. In formal terms:\n",
    "$$\n",
    "P(x|x_{i}) = \\begin{cases}\n",
    "1  &  \\text{if} &  x=x_{i+1} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "Since we're conditioning our model probability distribution on predicting $x_{i+1}$ we can say that $P(x_{i+1}|x_{i})=1$ and therefore the loss we have to minimize is:\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{i=1}^{n-1} \\log P(x_{i+1}|x_{i},\\Theta)\n",
    "$$\n",
    "In practice and mostly for computational reasons we instead calculate the loss over mini batches $B$ where $B \\subset X$ (a random selection of training examples), therefore the loss we will actually use is:\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{|B|} \\sum_{i \\in B} \\log (\\text{softmax} (f_{\\Theta}(x_{i}) )_{y_{i}})\n",
    "$$\n",
    "Minimizing this loss amounts to maximizing the probability of seeing the index $y_{i}$ (correct next character according to our data) when out model takes in the character $x_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2562e",
   "metadata": {},
   "source": [
    "## Network y Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae6b00ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10918891]), torch.Size([10918891]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "data = torch.tensor(bigrams)\n",
    "X = data[:, 0]\n",
    "Y = data[:, 1]\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "958c092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "hidden_dim = 64\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "embed = torch.randn(vocab_size, embed_dim)\n",
    "hidden_ff = torch.randn(embed_dim, hidden_dim)\n",
    "out_ff = torch.randn(hidden_dim, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99141962",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = torch.randint(0, data.shape[0], (batch_size,))\n",
    "X_batch = X[batch_idx]\n",
    "X_embed = embed[X_batch]\n",
    "\n",
    "hidden_logits = X_embed @ hidden_ff\n",
    "hidden_act = torch.tanh(hidden_logits)\n",
    "\n",
    "out_logits = hidden_act @ out_ff\n",
    "\n",
    "probs = F.softmax(out_logits, dim=-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5348ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.5534)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_batch = Y[batch_idx]\n",
    "\n",
    "# We pluck out the probability of y_i (correct next character) from our probs and compare against the real probability\n",
    "correct_logprobs = torch.log(probs[torch.arange(batch_size), Y_batch]) \n",
    "\n",
    "loss = -correct_logprobs.mean()\n",
    "\n",
    "loss # Horrible to start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee0619",
   "metadata": {},
   "source": [
    "## Using Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647713e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
